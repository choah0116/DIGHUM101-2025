{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Challenge 1: More data structures - Array\n",
    "\n",
    "Lists are called arrays when they contain data of the same type, and may be preferred to lists when handling large amounts of data because they are computationally more efficient. Use your numpy library to create a \"numpy array\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a numpy array\n",
    "my_array = np.array([4, 5, 6, 10])\n",
    "\n",
    "print(my_array)\n",
    "print(type(my_array))"
=======
    "# Python Data Wrangling with `pandas`: Part 2\n",
    "\n",
    "* * * \n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Gain familiarity with `pandas` and the core `DataFrame` object\n",
    "* Apply core data wrangling techniques in `pandas`\n",
    "* Understand the flexibility of the `pandas` library\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "ü•ä **Challenge**: Interactive exercise. We'll work through these in the workshop!<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "üé¨ **Demo**: Showing off something more advanced ‚Äì so you know what Pandas can be used for!<br>"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Like lists, we can change an individual value by reassigning its index:"
=======
    "### Sections\n",
    "4. [Missing Data](#missing)\n",
    "5. [Sorting Values](#sorting)\n",
    "6. [Merging](#merging)\n",
    "7. [Grouping](#grouping)\n",
    "8. [Visualization](#viz)\n",
    "\n",
    "Let's start back up by importing our libraries and loading up our data"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "my_array[0] = 42\n",
    "print(my_array)"
=======
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the unemployment dataset\n",
    "unemployment = pd.read_csv('../Data/cleaned_country_totals.csv')\n",
    "# This is some formatting that's out of scope\n",
    "unemployment['date'] = pd.to_datetime(unemployment['date'])\n",
    "\n",
    "# Open the countries dataset\n",
    "countries_url = 'https://raw.githubusercontent.com/dlab-berkeley/Python-Data-Wrangling/main/data/countries.csv'\n",
    "countries = pd.read_csv(countries_url)"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Define an array based on a range of numbers:"
=======
    "<a id='missing'></a>\n",
    "# Missing Values\n",
    "When working with a new data source, it's good to get an idea of how much information is missing. `pandas` provides various methods for exploring and dealing with \"missing-ness\", one of which is `.isna()`"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "## YOUR CODE HERE\n",
=======
    "unemployment.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.isna()` method returns a corresponding boolean value for each entry in the `unemployment` DataFrame. In Python, `True` is equivalent to 1 and `False` is equivalent to 0. Thus, when we add up the results by column (with `.sum()`), we get a count for the **total** number of missing values by column::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very helpful trick, and it shows us that the only column with missing values is `unemployment_rate`.\n",
    "\n",
    "There are a wide variety of approaches to dealing with missing data. One basic approach would be to drop any row with a missing unemployment rate record:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment.dropna(subset=['unemployment_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply running the `.dropna()` method actually doesn't alter our data -- it makes a copy then drops the missing rows for that copy. In order to save our alteration, we'll need to re-define the `unemployment` DataFrame with our altered copy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment = unemployment.dropna(subset=['unemployment_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° Tip:** Note that the brackets `[]` are needed in the `dropna()` method because the subset parameter expects a list of column names. Even if you're specifying just one column, it still needs to be enclosed in brackets to indicate that it's a list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sorting'></a>\n",
    "# Sorting Values\n",
    "\n",
    "We've been working with data about unemployment rates, so it would probably be useful to know what the highest unemployment rates are in this data. For this, we'll use the `sort_values()` method to sort the data. We chain `.head()` onto the end of this so that we only see the first five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment.sort_values(by='date', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code creates a copy of the `DataFrame`, sorted in **descending** order (note `ascending=False`), and prints the first five rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 5\n",
    "Let's use sorting to answer a practical question:  \n",
    "\n",
    "<span style=\"color:purple\">Which country has the highest unemployment rate in our data?  \n",
    "</span>\n",
    "<details><summary><a>Click for hint</a></summary>\n",
    "1. Use <code>.sort_values()</code> to sort this data based on the unemployment rate using descending order<br>\n",
    "2. Select the top row using <code>.head()</code> with an argument for number of rows<br>\n",
    "</details>\n",
>>>>>>> upstream/main
    "\n"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if I want just the third, fourth and fifth elements of \"number_range\""
   ]
  },
  {
=======
>>>>>>> upstream/main
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "## YOUR CODE HERE\n",
    "\n"
=======
    "# YOUR CODE HERE"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Challenge 2: More data structures - Series\n",
    "\n",
    "Series are the pandas version of arrays - one dimensional data storage, preferably of the same type."
=======
    "<a id='merging'></a>\n",
    "# Merging DataFrames\n",
    "\n",
    "Our `unemployment` has a lot of interesting information, but it is unfortunately missing any full country names. We have that information in the `countries` DataFrame, but we need to find a way of combining it with `unemployment`."
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "pd.Series?"
=======
    "countries.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the data we need is stored in two separate files, we'll want to merge the data somehow. Let's determine which column we can use to join this data by taking a look at `unemployment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment.head()"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "What must you do before you can call the help files for `pd.Series`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series?"
=======
    "These two DataFrames seem to have a similar `country` column, with two character codes for each country. Let's try doing a merge of these two datasets based on the `country` column."
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "What are the two arguments in the `pd.Series` code below?"
=======
    "`pandas` includes an easy-to-use merge function that accepts two DataFrames and a column to merge them on:"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd_series = pd.Series([1, 2, 3, 4, 5], \n",
    "                      index = [\"a\", \"b\", \"c\", \"D\", \"E\"])\n",
    "print(pd_series)\n",
    "print(type(pd_series))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3: DataFrame\n",
    "\n",
    "A pandas DataFrame is an ordered group of equal-length series/arrays. This ensures that data are of the same type within each column, but that data can be of different types across rows (think like an MS Excel spreadsheet!)\n",
    "\n",
    "> Remember that pandas will be a large focus in Week 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame using lists\n",
    "\n",
    "# Define two lists\n",
    "heroes = [\"Superman\", \"Black Panther\", \"Wonder Woman\", \"Batman\", \"Storm\"]\n",
    "hometown = [\"Krypton\", \"Wakanda\", \"Themyscira\", \"Gotham\", \"Harlem\"]\n",
    "age = [23, 25, 41, 52, 22]\n",
    "\n",
    "# Create an empty DataFrame\n",
    "comics = pd.DataFrame()\n",
    "print(comics)"
=======
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment_merged = pd.merge(unemployment, countries, on='country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment_merged.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='grouping'></a>\n",
    "# Grouping and Aggregating Data\n",
    "\n",
    "What if we'd like to know how many observations exist for each country? To do so, we need to group the countries, then count how many times each one occurs. In other words, we're going to **group** our data **by** a specific column, and calculate some quantity within each group. The \"group-by\" operation is a fundamental technique used with tabular data.  \n",
    "\n",
    "\n",
    "(For those who have used spreadsheet software like Excel, you might recognize that we are essentially talking about making a \"Pivot-Table\")\n",
    "\n",
    "## Simple Grouping with `.value_counts()`\n",
    "For simple grouping operations, we can use the handy `.value_counts()` method. We typically run this on a single column, and it will return a table showing how many observations there are for each unique value in the column. The following graphic represents the basics of the operations.\n",
    "<img src=\"../images/valcounts.svg\" align=\"left\" width=\"400\" alt=\"diagram of pandas datafram\">"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# Use the lists to define the columns\n",
    "comics[\"Name\"] = heroes\n",
    "comics[\"Home\"] = hometown\n",
    "comics[\"Age\"] = age\n",
    "\n",
    "# View the data frame\n",
    "comics"
=======
    "unemployment_merged['name_en'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that we have a lot of observations of data for Italy, France, Sweden, etc. and very few observations for Turkey and Estonia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 6\n",
    "Try using `.value_counts()` on the DataFrame to find out how many observations are from EU versus non-EU records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex grouping with `.groupby()`\n",
    "What if we want to do something more complex, like find out **what was the average unemployment rate for EU and non-EU countries?**. `.value_counts()` groups data then counts it, but we need a method that can group data then average it.\n",
    "\n",
    "This sort of question is a typical use case for `.groupby()` -- which allows us to group data then apply any **aggregate** function we want -- count, average, min, max, median, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, we want to find out the average unemployment rate for EU and non-EU countries, so we will group our data based on `country_group`. Here is a graphical representation of our goal:    \n",
    "<img src=\"../images/groupby.svg\" align=\"left\" width=\"500\" alt=\"diagram of pandas datafram\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the method, `.groupby()`. This doesn't actually return data or output -- it just groups the data.  "
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "print(type(list_df[\"AGE\"]))"
=======
    "unemployment_merged.groupby('country_group')"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### `zip()`\n",
    "\n",
    "The `zip()` function is a handy way to define data frames using tuples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using zip()\n",
    "\n",
    "# Define lists to be the columns in our data frame\n",
    "day = [\"Monday\", \"Tuesday\", \"Friday\"]\n",
    "temp = [88, 91, 67]\n",
    "\n",
    "# Convert our two separate lists into a list of tuples\n",
    "tuples = list(zip(day, temp))\n",
    "tuples\n",
    "\n",
    "# Create the data frame \n",
    "pd.DataFrame(tuples, columns=[\"Day\", \"Temp\"])"
=======
    "We now have to select a column of data and specify an aggregate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment_merged.groupby('country_group')['unemployment_rate'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dissecting the code, we told `pandas`:\n",
    "1. <code>unemployment_merged<code><mark style=\"background: yellow\">.groupby('country_group')</mark>['unemployment_rate'].mean()</code>\n",
    "\n",
    "    Group all of our rows based on the unique values of the `country_group` column -- EU, non-EU\n",
    "    \n",
    "2. <code>unemployment_merged<code>.groupby('country_group')<mark style=\"background: yellow\">['unemployment_rate']</mark>.mean()</code>\n",
    "\n",
    "    Select the `unemployment_rate` column\n",
    "\n",
    "2. <code>unemployment_merged<code>.groupby('country_group')['unemployment_rate']<mark style=\"background: yellow\">.mean()</mark></code>\n",
    "\n",
    "   Compute the average of the selected column (`unemployment_rate`) for each group"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Create a small data frame like the ones above from scratch. Be creative! "
=======
    "We can confirm this behavior using boolean indexing as well. If we index to only those records from EU countries, select the `unemployment_rate` column, then compute the average, we should get 8.3, the same value computed with groupby:"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "## YOUR CODE HERE\n",
    "\n"
=======
    "boolean_index = unemployment_merged['country_group'] == 'eu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment_merged.loc[boolean_index, 'unemployment_rate'].mean()"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "In `pd.DataFrame`, what does index and columns do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The remaining challenges are designed to challenge you beyond what we have covered so far. If you feel frustrated and do not understand, that is normal! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 4\n",
    "\n",
    "1. [Visit www.mockaroo.com](https://mockaroo.com/) and click the \"Download Data\" button to generate a simulated dataset.\n",
    "\n",
    "2. Import this example data into a Pandas DataFrame.\n",
    "\n",
    "> HINT: First, copy your new .csv file into your working directory! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 5: XML\n",
    "\n",
    ".csv files are nice when we know the structure of the data (rows and columns) that we can quickly perform operations on. However, we are limited to this format, which can be difficult to incorporate into nested, hierarchical data structures. \n",
    "\n",
    "[Extensible Markup Language (.xml)](https://www.sitepoint.com/really-good-introduction-xml/) is good for representing data that have a hierarchical structure. Large, web-based datasets might be stored this way, and we can use their human readable format to select tags that we want to extract for analysis. However, these data are larger than .csv files because of the opening and closing \"tags\". \n",
    "\n",
    "There are a few libraries in Python that will parse XML for you, but we'll stick to the standard `xml` library in this course. Let's start out with a simple example. Suppose we want to store the metadata for the books in our library in an XML file. It might look something like this:"
=======
    "The strengths of `.groupby()` relative to using boolean indexing are that `groupby()` scales very well to scenarios with many groups, and it requires much less code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 7\n",
    "\n",
    "Use `.groupby()` to find the maximum unemployment rate for each country. Sort your results from largest to smallest.\n",
    "\n",
    "Use the example above for guidance. \n",
    "<details><summary><a>Click for hint</a></summary>\n",
    "1. First, use <code>groupby()</code> to group on \"name_en\". <br>\n",
    "2. Then, select the \"unemployment_rate\" column,<br>\n",
    "3. Aggregate by using <code>.max()</code> to get the max value.<br>\n",
    "4. Chain on the method <code>.sort_values(ascending=False)</code>.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='viz'></a>\n",
    "# Data Visualization\n",
    "In the last challenge, you created an interesting table showing the all-time maximum unemployment rate that each country has experienced. Let's visualize that table as a bar chart to make it easier to present.\n",
    "\n",
    "There are various ways to approach data visualization in Python -- we'll cover simple plotting in `pandas`, which draws on functionality from the `matplotlib` library.\n",
    "\n",
    "First, we'll define a variable, `grouped`, with the table you just made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = unemployment_merged.groupby('name_en')['unemployment_rate'].max().sort_values(ascending=False)\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot it. In `pandas`, visualization is as simple as calling the `.plot()` method, then supplying optional arguments (here I supplied `kind=\"barh\"` to make a horizontal bar chart rather than the default line-chart). The following is the maximum unemployment rate across countries in the data for each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.plot(kind='barh')\n",
    "\n",
    "# We add plt.show() to properly render the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another plot. Our full DataFrame is what's called a time-series -- we have repeated observations of various countries' unemployment rates over time. We typically plot time-series using line-plots, so let's make a line plot examining Spain and Portugal's unemployment rates.  \n",
    "\n",
    "To make this sort of plot simpler, we'll start by making our date column into the DataFrame's index:"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "pwd"
=======
    "unemployment_merged = unemployment_merged.set_index('date')"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "import os\n",
    "os.chdir(\"../../../Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat example.xml"
=======
    "unemployment_merged.head()"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "To parse this in Python, we'll need to import the `xml` library and then build the tree. From the `tree` object, we can get the root of the tree, and then traverse the tree like we would our filesystem paths."
=======
    "We will also use boolean indexing to select on those observations that are for Spain. We'll save that as a new DataFrame, `spain`:"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "import xml.etree.ElementTree as ET # loads functions from `xml` library under the name `ET`\n",
    "\n",
    "tree = ET.parse(\"example.xml\")\n",
    "root = tree.getroot()\n",
    "print(ET.tostring(root))"
=======
    "spain = unemployment_merged[unemployment_merged['name_en'] == 'Spain']\n",
    "spain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily access Spain's unemployment rate, with the date for each observation included as the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spain['unemployment_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the same boolean indexing we used for Spain, but now for Portugal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal = unemployment_merged[unemployment_merged['name_en'] == 'Portugal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portugal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take advantage of the `.plot()` function, simply calling `spain['unemployment_rate'].plot()`. We don't need to supply any argument to `.plot()` since we are using the default plot style -- a line-plot. We do add some other commands to add a y-axis label and render the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spain['unemployment_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spain['unemployment_rate'].plot()\n",
    "\n",
    "# We add plt.show() to properly render the chart\n",
    "plt.show()"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "This is how it would look if we typed it out:"
=======
    "Layering plots will involve simply calling multiple `.plot()` commands in the same Jupyter cell. We can add some basic styling as well, such as labels, a legend, and a title."
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "xml_string = '''\n",
    "<my-library>\n",
    "    <book>\n",
    "        <title>The Lion, the Witch and the Wardrobe</title>\n",
    "        <author>C. S. Lewis</author>\n",
    "        <date>1950</date>\n",
    "        <publisher>Geoffrey Bles</publisher>\n",
    "    </book>\n",
    "    <book>\n",
    "        <title>The Hobbit</title>\n",
    "        <author>J. R. R. Tolkien</author>\n",
    "        <date>1937</date>\n",
    "        <publisher>George Allen and Unwin</publisher>\n",
    "    </book>\n",
    "    <book>\n",
    "        <title>To Kill A Mockingbird</title>\n",
    "        <author>Harper Lee</author>\n",
    "        <date>1960</date>\n",
    "        <publisher>J. B. Lippincott and Co.</publisher>\n",
    "    </book>\n",
    "</my-library>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = ET.fromstring(xml_string)\n",
    "print(ET.tostring(root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the direct children of the root with the `getchildren()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.getchildren()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`getchildren()` will always yield the elements subordinate to the parent element in a hierarchy. If we look at the XML string above, we can get the children of a `book` elements as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_book = root.getchildren()[0]\n",
    "first_book.getchildren()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `find` to quickly find an element. We'll use a for-loop to print the author for each book, which we get with the `find` method for each of the elements in the children above, and we get the text with the `text` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More on for-loops in Week 2\n",
    "for book in root:\n",
    "    print(book.find('author').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in XML you can have elements which are usually between the two `< >` signs and can be found using the `find` or `getchildren` methods. To get the actual information, or text, from that element you can use the `.text` property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find an .xml dataset on the web and import it into your Jupyter Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 6: XML Wikipedia data\n",
    "\n",
    "Let's work with a real-world dataset. We're going to work with the revision history of a Wikipedia page. You can get this through Wikipedia's [API](https://www.mediawiki.org/wiki/API:Revisions) or [download](https://en.wikipedia.org/wiki/Wikipedia:Database_download) data directly. We'll look at the API more in a later module, so we've downloaded a page of revisions and saved it to the XML file in `../data/WIKIPEDIA/feminism.xml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head feminism.xml"
=======
    "# Plot commands\n",
    "spain['unemployment_rate'].plot()\n",
    "portugal['unemployment_rate'].plot()\n",
    "\n",
    "# Styling\n",
    "plt.legend([\"Spain\", \"Portugal\"])\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Spain and Portugal Unemployment Rates\")\n",
    "plt.show()"
>>>>>>> upstream/main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "We can start off by parsing it like we did above with our books:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse(\"feminism.xml\")\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you can either look in the XML in a text editor, or start the process of looking through the tree manually to find the actual metadata of each revision. You'll have to do this when we work with JSON in a later challange, so don't be afraid if none of this makes sense at this point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.getchildren()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.find('query').getchildren()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.find('query/pages').getchildren()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.find('query/pages/page').getchildren()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.find('query/pages/page/revisions').getchildren()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found it! Those are the revision items we want. Let's assign that list to a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revisions = root.find('query/pages/page/revisions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can loop through and ask for some of the specific metadata about each revision, the `timestamp` for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rev in revisions.getchildren():\n",
    "    print(rev.get('timestamp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 7: JSON\n",
    "\n",
    "As a Pythonista, after dealing with XML you'll be happy to see [JSON](https://en.wikipedia.org/wiki/JSON). JavaScript Object Notation (JSON) is preferred because it looks exactly like a Python `dictionary`. The `json` library takes care of the few differences between the two so that we don't have to ourselves. Let's take a look at the same data about the books in our library from the XML notebook in JSON format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat example.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks much nicer than XML and its many tags! In Python, it looks like a `dictionary` with a key of `my_library` and a value of a `list` of `dictionary` objects. Each of these `dictionary` objects in the list contains the metadata. We can use the `json` library to make it exactly that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "my_library = json.load(open(\"example.json\"))\n",
    "my_library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(my_library))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index `my_library` to return only the year \"The Hobbit\" was published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will visualize some XML and JSON data next week!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 8: JSON Wikipedia Data\n",
    "\n",
    "Let's go back to that revision history data. Wikipedia actually prefers that you get the data in JSON format, so we've downloaded some more data for you in JSON, it's located at `../data/WIKIPEDIA/feminism.json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feminism_json = json.load(open(\"feminism.json\"))\n",
    "feminism_json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feminism_json[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feminism_json[0]['timestamp']"
=======
    "# üéâ Well Done!\n",
    "\n",
    "This workshop series took us through the basics of data analysis using `pandas`. We indexed data using boolean logic, grouped data to perform conditional analyses, and we create basic but informative visualizations.\n",
    "\n",
    "## More Workshops!\n",
    "\n",
    "D-Lab teaches workshops that allow you to practice more with DataFrames and visualization.\n",
    "\n",
    "- To learn other fundamental Python topics, check out D-Lab's [Python Intermediate workshop](https://github.com/dlab-berkeley/Python-Intermediate-Pilot).\n",
    "- To learn more about data visualization, check out D-Lab's [Python Data Visualization workshop](https://github.com/dlab-berkeley/Python-Data-Visualization)."
>>>>>>> upstream/main
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
=======
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
>>>>>>> upstream/main
}
