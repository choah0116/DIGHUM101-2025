{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>DIGHUM101</b></center>\n",
    "<center>2-1: Text Preprocessing</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast review\n",
    "\n",
    "1. What is a data frame? \n",
    "2. What methods and other syntax can be used to subset rows and columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning objectives\n",
    "\n",
    "1. Download a .txt file from Project Gutenberg and import it into Python\n",
    "2. Quick walkthrough of Hathi Trust Research Center (HTRC) resources\n",
    "3. Learn the basics of text preprocessing: \n",
    "    - Tokenization\n",
    "    - Punctuation removal\n",
    "    - Count words, unique words, and word frequencies\n",
    "    - Stop word removal\n",
    "    - Stemming/lemmatization\n",
    "    - Part of speech tagging\n",
    "    - Quick introduction to n-grams, skip-grams, and BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# Module to remove punctuation from string library\n",
    "from string import punctuation\n",
    "print(punctuation)\n",
    "print(len(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module to count word frequencies\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ahyeoncho/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ahyeoncho/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "# Module to help us remove stopwords\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp312-cp312-macosx_10_13_x86_64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp312-cp312-macosx_10_13_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-macosx_10_13_x86_64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp312-cp312-macosx_10_13_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp312-cp312-macosx_10_13_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-macosx_10_13_x86_64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ahyeoncho/Downloads/miniconda3/envs/dighum101/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/ahyeoncho/Downloads/miniconda3/envs/dighum101/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ahyeoncho/Downloads/miniconda3/envs/dighum101/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/ahyeoncho/Downloads/miniconda3/envs/dighum101/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Users/ahyeoncho/Downloads/miniconda3/envs/dighum101/lib/python3.12/site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ahyeoncho/Downloads/miniconda3/envs/dighum101/lib/python3.12/site-packages (from spacy) (25.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.33.2-cp312-cp312-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/ahyeoncho/Downloads/miniconda3/envs/dighum101/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/ahyeoncho/Downloads/miniconda3/envs/dighum101/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ahyeoncho/Downloads/miniconda3/envs/dighum101/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ahyeoncho/Downloads/miniconda3/envs/dighum101/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ahyeoncho/Downloads/miniconda3/envs/dighum101/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp312-cp312-macosx_10_13_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Downloading numpy-2.2.6-cp312-cp312-macosx_14_0_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/ahyeoncho/Downloads/miniconda3/envs/dighum101/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/ahyeoncho/Downloads/miniconda3/envs/dighum101/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /Users/ahyeoncho/Downloads/miniconda3/envs/dighum101/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-macosx_10_13_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/ahyeoncho/Downloads/miniconda3/envs/dighum101/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ahyeoncho/Downloads/miniconda3/envs/dighum101/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Downloading spacy-3.8.7-cp312-cp312-macosx_10_13_x86_64.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-macosx_10_13_x86_64.whl (42 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp312-cp312-macosx_10_13_x86_64.whl (26 kB)\n",
      "Downloading preshed-3.0.10-cp312-cp312-macosx_10_13_x86_64.whl (131 kB)\n",
      "Downloading pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp312-cp312-macosx_10_12_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-macosx_10_13_x86_64.whl (636 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.7/636.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.6-cp312-cp312-macosx_10_13_x86_64.whl (890 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.2/890.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading blis-1.3.0-cp312-cp312-macosx_10_13_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading numpy-2.2.6-cp312-cp312-macosx_14_0_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading marisa_trie-1.2.1-cp312-cp312-macosx_10_13_x86_64.whl (190 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: cymem, wasabi, typing-inspection, spacy-loggers, spacy-legacy, shellingham, pydantic-core, numpy, murmurhash, mdurl, marisa-trie, cloudpathlib, catalogue, annotated-types, srsly, pydantic, preshed, markdown-it-py, language-data, blis, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "\u001b[2K  Attempting uninstall: numpym\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/27\u001b[0m [pydantic-core]ion]\n",
      "\u001b[2K    Found existing installation: numpy 1.26.4━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/27\u001b[0m [numpy]ore]\n",
      "\u001b[2K    Uninstalling numpy-1.26.4:0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/27\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.26.4━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/27\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27/27\u001b[0m [spacy]m26/27\u001b[0m [spacy]]ion]ta]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.13 numpy-2.2.6 preshed-3.0.10 pydantic-2.11.5 pydantic-core-2.33.2 rich-14.0.0 shellingham-1.5.4 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.16.0 typing-inspection-0.4.1 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    }
   ],
   "source": [
    "# Install spaCy and trained model downloaded.\n",
    "# install spacy\n",
    "!pip install spacy\n",
    "\n",
    "# Download a trained English model (small)\n",
    "# !python -m spacy download en_core_web_sm \n",
    "\n",
    "# Download the large model as well\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Gutenberg\n",
    "\n",
    "[Project Gutenberg](https://www.gutenberg.org/) has more than 60,000 texts for you to download. Be sure to check out their [Terms of Use](https://www.gutenberg.org/wiki/Gutenberg:Terms_of_Use). You can find many .txt files here that are in the public domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ahyeoncho/Desktop/DIGHUM101-2025/Week 2'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try it! Search for a book, download it, copy it to your working directory, and import it.\n",
    "\n",
    "## YOUR CODE HERE\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../Data/'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../../Data/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mls\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../../Data/'"
     ]
    }
   ],
   "source": [
    "os.chdir(\"../../Data/\")\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dracula.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## HOW TO IMPORT dracula.txt?\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dracula = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdracula.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.read()\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(dracula[\u001b[32m501\u001b[39m:])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/miniconda3/envs/dighum101/lib/python3.12/site-packages/IPython/core/interactiveshell.py:327\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    321\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    322\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    325\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'dracula.txt'"
     ]
    }
   ],
   "source": [
    "## HOW TO IMPORT dracula.txt?\n",
    "dracula = open(\"dracula.txt\").read()\n",
    "print(dracula[501:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hathi Trust Research Center (HTRC)\n",
    "\n",
    "Check out the [HTRC](https://www.hathitrust.org/) and learn about their many [collections tools](https://www.hathitrust.org/htrc_collections_tools) and the [Python library](https://github.com/htrc/htrc-feature-reader) to connect to the API. The [Analytics](https://analytics.hathitrust.org/) website gives you access to many canned features if you don't want to mess with the Python code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing: Strings in depth\n",
    "\n",
    "Text preprocessing is an essential first step to coding and understanding machine learning algorithms. For machine learning portions of this course, we will focus on [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) models, namely [document-term](https://en.wikipedia.org/wiki/Document-term_matrix) and [term frequency-inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) matrices from the [sklearn library](https://scikit-learn.org/stable/).\n",
    "\n",
    "Text preprocessing/pattern matching can be further enhanced through use of [regular expressions](https://docs.python.org/2/library/re.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![borges](../../Img/borges_1921.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "\n",
      "In the fullness of the years, like it or not,\n",
      "a luminous mist surrounds me, unvarying, \n",
      "that breaks things down into a single thing,\n",
      "colorless, formless. Almost into a thought. \n",
      "The elemental, vast night and the day\n",
      "teeming with people have become that fog\n",
      "of constant, tentative light that does not flag,\n",
      "\n",
      "and lies in wait at dawn. I longed to see\n",
      "just once a human face. Unknown to me\n",
      "the closed encyclopedia, the sweet play\n",
      "in volumes I can do no more than hold, \n",
      "the tiny soaring birds, the moons of gold.\n",
      "Others have the world, for better or worse; \n",
      "I have this half-dark, and the toil of verse.\n"
     ]
    }
   ],
   "source": [
    "borges = '''In the fullness of the years, like it or not,\n",
    "a luminous mist surrounds me, unvarying, \n",
    "that breaks things down into a single thing,\n",
    "colorless, formless. Almost into a thought. \n",
    "The elemental, vast night and the day\n",
    "teeming with people have become that fog\n",
    "of constant, tentative light that does not flag,\n",
    "\n",
    "and lies in wait at dawn. I longed to see\n",
    "just once a human face. Unknown to me\n",
    "the closed encyclopedia, the sweet play\n",
    "in volumes I can do no more than hold, \n",
    "the tiny soaring birds, the moons of gold.\n",
    "Others have the world, for better or worse; \n",
    "I have this half-dark, and the toil of verse.'''\n",
    "\n",
    "\n",
    "print(type(borges))\n",
    "print()\n",
    "print(borges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do the triple quotes do in the assignment of borges above?\n",
    "\n",
    "# Also, make a copy to preserve the original borges variable\n",
    "poem = borges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is the process of splitting text into _something_ - often words. Each word is called a \"token\" and a word such as \"the\" might adhere to multiple tokens of \"the\" within a text based on its capitalization, punctuation, etc.\n",
    "\n",
    "The `.split()` method allows us to split the text based on some sort of separator. The default is blank and will split on the blank spaces between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'the', 'fullness', 'of', 'the', 'years,', 'like', 'it', 'or', 'not,', 'a', 'luminous', 'mist', 'surrounds', 'me,', 'unvarying,', 'that', 'breaks', 'things', 'down', 'into', 'a', 'single', 'thing,', 'colorless,', 'formless.', 'Almost', 'into', 'a', 'thought.', 'The', 'elemental,', 'vast', 'night', 'and', 'the', 'day', 'teeming', 'with', 'people', 'have', 'become', 'that', 'fog', 'of', 'constant,', 'tentative', 'light', 'that', 'does', 'not', 'flag,', 'and', 'lies', 'in', 'wait', 'at', 'dawn.', 'I', 'longed', 'to', 'see', 'just', 'once', 'a', 'human', 'face.', 'Unknown', 'to', 'me', 'the', 'closed', 'encyclopedia,', 'the', 'sweet', 'play', 'in', 'volumes', 'I', 'can', 'do', 'no', 'more', 'than', 'hold,', 'the', 'tiny', 'soaring', 'birds,', 'the', 'moons', 'of', 'gold.', 'Others', 'have', 'the', 'world,', 'for', 'better', 'or', 'worse;', 'I', 'have', 'this', 'half-dark,', 'and', 'the', 'toil', 'of', 'verse.']\n"
     ]
    }
   ],
   "source": [
    "# Split the string into a list of strings (single words)\n",
    "print(poem.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count words\n",
    "\n",
    "Jump in! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many characters in poem?\n",
    "len(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many words?\n",
    "len(poem.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many lines?\n",
    "len(poem.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many periods? \n",
    "# Should this be equal to the number of sentences in the cell below?\n",
    "poem.count(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In the fullness of the years, like it or not,\\na luminous mist surrounds me, unvarying, \\nthat breaks things down into a single thing,\\ncolorless, formless',\n",
       " ' Almost into a thought',\n",
       " ' \\nThe elemental, vast night and the day\\nteeming with people have become that fog\\nof constant, tentative light that does not flag,\\n\\nand lies in wait at dawn',\n",
       " ' I longed to see\\njust once a human face',\n",
       " ' Unknown to me\\nthe closed encyclopedia, the sweet play\\nin volumes I can do no more than hold, \\nthe tiny soaring birds, the moons of gold',\n",
       " '\\nOthers have the world, for better or worse; \\nI have this half-dark, and the toil of verse',\n",
       " '']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ... but how many sentences? Why is this different from the number of periods?\n",
    "len(poem.split(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many stanzas?\n",
    "len(poem.split(\"\\n\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# At which index does the word \"me\" first appear?\n",
    "# .find() is \"forward search\"\n",
    "poem.find(\"me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .index works as well\n",
    "poem.index(\"me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that .find does not throw an error when an element is not found (but .index does)\n",
    "poem.find(\"kangaroo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At which index does the word \"me\" last appear?\n",
    "# .rfind() starts at the highest index and works in reverse\n",
    "poem.rfind(\"me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count _unique_ words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique words?\n",
    "# \"Casting\" our list into a set\n",
    "len(set(poem.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why two less unique words when we convert all the text to lower?\n",
    "len(set(poem.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the unique words\n",
    "print(set(poem.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What type of data structure is this? \n",
    "type(set(poem.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why is this different from .lower()?\n",
    "len(set(poem.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punctuation removal \n",
    "\n",
    "Remember how we imported that nice string of English punctuation in the first cell of this notebook? We could manually remove all of the punctuation using the .replace method, but this would get old fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many characters\n",
    "len(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace periods with nothing\n",
    "del_periods = poem.replace(\".\", \"\")\n",
    "del_periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, what if you have tons of text and don't know exactly what punctuation is present? A quick comprehension can help us remove all the punctuation from dirty, i.e. !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~)\n",
    "\n",
    "> NOTE: You will learn more about custom functions, for loops, list comprehensions, and lambda functions starting in week 3. The reason we are glossing over them now is so that you focus on **what is possible** for planning your individual projects instead of getting lost and frustrated in the nuances of the Python code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop\n",
    "for char in punctuation:\n",
    "    poem = poem.lower().replace(char, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Punctuation is gone! \n",
    "print(poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize poem into single words\n",
    "tokens = poem.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the ten most common words (stopwords included)\n",
    "freq = Counter(tokens)\n",
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop word removal\n",
    "\n",
    "[Stop words](https://en.wikipedia.org/wiki/Stop_words) are the most common words in a language, and may or may not add information about the content of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words(\"english\")\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List comprehension (we also saw for converting to datetime in \"2-1_pandas.ipynb\")\n",
    "no_stops = [word for word in tokens if word not in stopwords.words('english')]\n",
    "print(no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the same as the following:\n",
    "no_stops = []\n",
    "for word in tokens:\n",
    "    if word not in stopwords.words('english'):\n",
    "        no_stops.append(word)\n",
    "print(no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq2 = Counter(no_stops)\n",
    "freq2.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization\n",
    "\n",
    "One common problem with standardizing text is standardizing all parts of a word to its root, stem, or prefix. This is useful as it allows us to analyze word meaning without having to pour over separate inflectional forms of a word. It also speeds up the process as we have fewer words.\n",
    "\n",
    "[What's the difference between stemming and lemmatizing?](https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming)\n",
    "\n",
    "\"**Stemming** usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. **Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma. \n",
    "\n",
    "If confronted with the token \"saw\", **stemming** might return just \"s\", whereas **lemmatization** would attempt to return either \"see\" or \"saw\" depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma.\" See [this post](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) for more information.\n",
    "\n",
    "So what do we do? Use pretrained models from [spaCy](https://spacy.io/)! It does all the tokenization and lemmatization for you.\n",
    "\n",
    "[Read more about spaCy's pretrained models](https://spacy.io/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing with spaCy\n",
    "\n",
    "Let's start by loading the trained model. We don't need the Named Entity Recognition (NER) or the text classification capabilities of the model, so we don't load them to make everything faster. If we wanted to lemmatize a language other than English, we would just need to download a trained model for that language using spaCy and change the 'en_core_web_sm' to the name of that model. Everything else would be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the small pretrained model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\"])\n",
    "print(type(nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy expects a string as input, so let's use .join to force our list into a string  \n",
    "words = ' '.join(tokens)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(words)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now get the lemma of any word using the .lemma_ attribute\n",
    "doc[5].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...Or even the part of speech\n",
    "doc[5].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that spaCy also has its own stopwords list\n",
    "nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the [spaCy documentation](https://spacy.io/api/token#attributes) for more information about all the linguistic features that spaCy allows you to access as attributes.\n",
    "\n",
    "Now, let's create a function that takes in a list of tokens and lemmatizes it using spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our function\n",
    "def lemmatize(tokens):\n",
    "    \"\"\"Return the lemmas for each word in `tokens`.\"\"\"\n",
    "    \n",
    "    # spacy models operate on strings, not lists, so we turn the tokens back into\n",
    "    # a string of words\n",
    "    words = ' '.join(tokens)\n",
    "    \n",
    "    # this line does all sorts of processing, including the lemmatization.\n",
    "    # `doc` will be like a list of tokens that we can iterate over\n",
    "    doc = nlp(words)\n",
    "    \n",
    "    # each token in `doc` holds information about that token. The `lemma_`\n",
    "    # attribute holds the lemma of that token represented as a string. For\n",
    "    # performance reasons, the `lemma` (without the trailing underscore) holds\n",
    "    # an integer representation of the token, that we'll rarely ever need.\n",
    "    return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lemmas = lemmatize(tokens)\n",
    "print(lemmas)\n",
    "\n",
    "# Notice that spacy lemmatizes pronouns (e.g. \"you\", \"I\", \"your\") in a funny way.\n",
    "# It just tells us that they are pronouns, rather than giving us something like\n",
    "# \"your\" -> \"you\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams, skip-grams, and BERT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you interested in tokenizing more than just single words for the purpose of increasing \"context\"? [n-grams](https://en.wikipedia.org/wiki/N-gram) are \"contiguous sequence of n items from a given sample of text or speech.\"\n",
    "\n",
    "[Check out this clever solution for n-gramizing text](https://stackoverflow.com/questions/17531684/n-grams-in-python-four-five-six-grams)\n",
    "\n",
    "Do you want even more context? We will learn more about [skip-grams](https://en.wikipedia.org/wiki/Word2vec) and [BERT](https://www.searchenginejournal.com/google-bert-update/332161/#close) later in this course. \n",
    "\n",
    "\n",
    "\"The skip-gram architecture weighs nearby context words more heavily than more distant context words.\"\n",
    "\n",
    "\"The BERT algorithm (Bidirectional Encoder Representations from Transformers) is a deep learning algorithm related to natural language processing. It helps a machine to understand what words in a sentence mean, but with all the nuances of context.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dighum101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
