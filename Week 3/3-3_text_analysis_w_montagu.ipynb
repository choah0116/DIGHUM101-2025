{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0bc999",
   "metadata": {},
   "source": [
    "# Lady Montagu's Letters - Continued\n",
    "\n",
    "In this notebook, we will continue our analysis of Lady Montagu's letters. We will run through some exercises to familiarize ourselves with some tools and concepts in computational text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b95d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from string import punctuation\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343724d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load our data\n",
    "montagu = pd.read_csv('../Data/montagu/montagu_letters_v2.csv')\n",
    "montagu.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f0c830",
   "metadata": {},
   "source": [
    "## Word and Other Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b8f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get all the letters from the dataset\n",
    "letters = montagu['body_cleaned'].tolist()\n",
    "print(len(letters))\n",
    "print(letters[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873235ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get word counts for each letter and save it to the dataframe\n",
    "\n",
    "montagu[\"word_count\"] = montagu[\"body_cleaned\"].apply(lambda text: len(word_tokenize(text)))\n",
    "\n",
    "# Let's take a quick look at the first few rows to verify the new column\n",
    "montagu[[\"body_cleaned\", \"word_count\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe91faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's get the number of sentences in the same way too\n",
    "\n",
    "montagu[\"sentence_count\"] = montagu[\"body_cleaned\"].apply(lambda text: len(sent_tokenize(text)))\n",
    "\n",
    "# Let's take a quick look at the first few rows to verify the new column\n",
    "montagu[[\"body_cleaned\", \"sentence_count\"]].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69075dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of statistics for both columns\n",
    "summary_stats = montagu[[\"word_count\", \"sentence_count\"]].describe()\n",
    "\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0646d82",
   "metadata": {},
   "source": [
    "## Interpretation of Word Count and Sentence Count Summary Statistics\n",
    "\n",
    "### Word Count:\n",
    "- **Count (58)**: There are 58 letters in the dataset, each with a corresponding word count.\n",
    "- **Mean (1358.28)**: The average word count across all letters is approximately **1358 words**. This indicates that, on average, each letter has about 1358 words.\n",
    "- **Standard Deviation (853.91)**: The standard deviation is quite high, suggesting a **wide spread** in word count. Some letters are significantly longer, while others are much shorter.\n",
    "- **Min (166)**: The shortest letter has **166 words**, indicating that there are some very short letters in the dataset.\n",
    "- **25th Percentile (743.25)**: 25% of the letters have fewer than **743 words**. This provides insight into the lower end of letter lengths.\n",
    "- **Median (1116.5)**: The median word count is **1116.5 words**, which means that half of the letters have fewer than 1116.5 words, and the other half have more. The **right skew** of the data (mean > median) suggests that some very long letters are influencing the average.\n",
    "- **75th Percentile (1758)**: 75% of the letters have fewer than **1758 words**.\n",
    "- **Max (4223)**: The longest letter has **4223 words**, which is a significant outlier compared to most of the other letters.\n",
    "\n",
    "### Sentence Count:\n",
    "- **Count (58)**: There are also 58 letters with sentence counts.\n",
    "- **Mean (48.43)**: The average sentence count is approximately **48 sentences** per letter, indicating that most letters are composed of just under 50 sentences.\n",
    "- **Standard Deviation (83.69)**: The large standard deviation suggests that the number of sentences per letter varies greatly, with some letters being much longer in terms of sentence count.\n",
    "- **Min (3)**: The letter with the fewest sentences contains only **3 sentences**.\n",
    "- **25th Percentile (19)**: 25% of the letters contain fewer than **19 sentences**.\n",
    "- **Median (31)**: The median sentence count is **31 sentences**, meaning that half of the letters contain fewer than 31 sentences, and half have more.\n",
    "- **75th Percentile (47)**: 75% of the letters have fewer than **47 sentences**.\n",
    "- **Max (637)**: The letter with the highest sentence count has **637 sentences**, which is an extreme outlier. This suggests that this letter is unusually detailed or lengthy.\n",
    "\n",
    "### Key Insights:\n",
    "- **Word Count**: The word count distribution is **right-skewed**, meaning most letters are relatively short, but there are a few very long letters (e.g., the maximum of 4223 words) that pull the average up. The letters generally fall between **166 and 1758 words**.\n",
    "- **Sentence Count**: Similarly, the sentence count distribution is **right-skewed**. The majority of the letters have between **20 and 50 sentences**, with a few extreme outliers (the maximum of 637 sentences).\n",
    "  \n",
    "### Overall Interpretation:\n",
    "- **Word Count**: The letters are generally moderate in length, but there are some very long outliers. The dataset contains mostly shorter letters, with a few long letters driving up the average word count.\n",
    "- **Sentence Count**: The sentence count is spread out, with most letters having between 20 and 50 sentences. However, a few letters contain significantly more sentences, suggesting that some letters are more detailed or formal than others.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad8948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the word counts and sentence counts across all letters\n",
    "# Plotting Word Count and Sentence Count Distributions with Seaborn\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Word Count Distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(montagu[\"word_count\"], kde=True, color='blue', bins=15)\n",
    "plt.title(\"Word Count Distribution\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Sentence Count Distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(montagu[\"sentence_count\"], kde=True, color='green', bins=15)\n",
    "plt.title(\"Sentence Count Distribution\")\n",
    "plt.xlabel(\"Sentence Count\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a49309",
   "metadata": {},
   "source": [
    "Now that we got a sense of the distributions, we can experiment with all of the letters together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1638897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the text data into a single string\n",
    "letter_texts = \" \".join(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485df7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokens = word_tokenize(letter_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de97e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and non-alphabetic tokens\n",
    "filtered_tokens = [word.lower() for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "filtered_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace4e10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a frequency distribution of words\n",
    "freq_dist = nltk.FreqDist(filtered_tokens)\n",
    "print(len(freq_dist))\n",
    "# Display the 10 most common words\n",
    "print(freq_dist.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da11c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the frequency distribution\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 6))\n",
    "freq_dist.plot(30, cumulative=False) # 30 most common words\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of tick-labels\n",
    "plt.grid(False)# no grid\n",
    "plt.title(\"Word Frequency Distribution in Montagu Letters\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827550e7",
   "metadata": {},
   "source": [
    "What do we think of this? What is this telling us about the contents of these letters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e1ac3d",
   "metadata": {},
   "source": [
    "## Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30304e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "# Generate a word cloud from the frequency distribution\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(freq_dist)\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.title(\"Word Cloud of Montagu Letters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a99e728",
   "metadata": {},
   "source": [
    "A bit too busy and rather hard to read. Let's change things around a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79330848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's take a closer look at what happens after the first 50 words\n",
    "# print(freq_dist.most_common(50)) # This is how we would display the first 50 most common words\n",
    "# now let's print the most common words after the first 50\n",
    "\n",
    "fifty_to_100 = freq_dist.most_common(100)[50:] # Display words from 51st to 100th most common\n",
    "for word , count in fifty_to_100:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6e2375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also look at the least common words\n",
    "least_common = freq_dist.most_common()[-50:]  # Get the last 50 least common words\n",
    "for word, count in least_common:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0822673b",
   "metadata": {},
   "source": [
    "All that is to say, it is not super straightforward to work with word frequencies, especially in a corpus of this medium size. \n",
    "\n",
    "But all is not lost! Let's go back to the letters and get rid of the words in our frequency distribution that appear in more than 50% of the letters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a93a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_in_letter_freq(letters):\n",
    "    \"\"\"\n",
    "    This function creates a frequency distribution that counts how many different letters each word appears in. \n",
    "    It uses a set to ensure that words are counted only once per letter.\n",
    "    \"\"\"\n",
    "    word_in_letter_freq = Counter()\n",
    "    \n",
    "    for letter in letters:\n",
    "        tokens = set(word_tokenize(letter.lower()))  # using set to count each word once per letter\n",
    "        for token in tokens:\n",
    "            if token.isalpha():  # ignore non-alphabetic tokens\n",
    "                word_in_letter_freq[token] += 1\n",
    "    \n",
    "    return word_in_letter_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda9e41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_common_words(freq_dist, word_in_letter_freq, letters, threshold=0.5):\n",
    "    \"\"\" \n",
    "    This function removes words from the frequency distribution (freq_dist) if they appear in more than 50% of the letters. \n",
    "    The threshold can be adjusted as needed.\n",
    "    \"\"\"\n",
    "    # Calculate the threshold for common words\n",
    "    total_letters = len(letters)\n",
    "    common_words = {word for word, count in word_in_letter_freq.items() if count / total_letters > threshold}\n",
    "    \n",
    "    # Remove common words from the frequency distribution\n",
    "    cleaned_freq_dist = {word: count for word, count in freq_dist.items() if word not in common_words}\n",
    "    \n",
    "    return cleaned_freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d576b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the word_in_letter_freq (frequency of word occurrence across letters)\n",
    "word_in_letter_freq = create_word_in_letter_freq(letters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dff143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the frequency distribution by removing common words\n",
    "cleaned_freq_dist = remove_common_words(freq_dist, word_in_letter_freq, letters, threshold=0.5)\n",
    "print(len(cleaned_freq_dist))  # Print the number of unique words after cleaning\n",
    "# Print the cleaned frequency distribution in descending order of frequency\n",
    "cleaned_freq_dist = dict(sorted(cleaned_freq_dist.items(), key=lambda item: item[1], reverse=True))\n",
    "print(\"Cleaned Frequency Distribution:\")\n",
    "for word, count in cleaned_freq_dist.items():\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74db05f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the cleaned frequency distribution in a plot\n",
    "# Let's plot the top 30 cleaned words\n",
    "top_30 = sorted(cleaned_freq_dist.items(), key=lambda x: x[1], reverse=True)[:30]\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(*zip(*top_30), color='darkblue')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.title(\"Top 30 Words in Montagu Letters After Cleaning\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of tick-labels\n",
    "plt.grid(False)  # no grid\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11222219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the same in a word cloud\n",
    "# Generate a word cloud from the cleaned frequency distribution\n",
    "wordcloud_cleaned = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(dict(top_30))\n",
    "# Display the cleaned word cloud\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud_cleaned, interpolation='bilinear')\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.title(\"Top 30 Words in Montagu Letters After Removing Words Common in More Than 50 percent of Letters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eb9df4",
   "metadata": {},
   "source": [
    "Bonus! We can change the colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "edb15a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom color paletter based on the official colors of UC Berkeley\n",
    "# https://brand.berkeley.edu/visual-identity/colors/\n",
    "\n",
    "color_palette = ['#002676', '#324262', '#655f4f', '#977b3b', '#ca9828', '#fdb515']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3bd7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a bit more finicy because we need to create a custom colormap\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "# Create a custom colormap\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"custom_palette\", color_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041c8af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's incorporate the custom color palette into the word cloud\n",
    "wordcloud_custom = WordCloud(width=800, height=400, background_color='white', colormap=custom_cmap).generate_from_frequencies(dict(top_30))\n",
    "# Display the word cloud with the custom color palette\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud_custom, interpolation='bilinear')\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.title(\"Top 30 Words in Montagu Letters with Custom Color Palette\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb78fc1c",
   "metadata": {},
   "source": [
    "Another alternative to counting words is counting bigrams\n",
    "\n",
    "Bigrams are pairs of words that occur side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c90d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a frequency distribution of the bigrams in the letters\n",
    "from nltk.util import bigrams\n",
    "\n",
    "# Initialize the Counter for bigram frequencies\n",
    "bigram_freq = Counter()\n",
    "\n",
    "# Iterate over the letters and extract bigrams\n",
    "for letter in letters:\n",
    "# Tokenize and lowercased words, removing non-alphabetic characters and stopwords\n",
    "    tokens = [word.lower() for word in word_tokenize(letter) if word.isalpha() and word not in stop_words]\n",
    "    bigram_freq.update(bigrams(tokens))  # Create and update bigrams\n",
    "\n",
    "# Display the 20 most common bigrams\n",
    "for bigram in bigram_freq.most_common(20):\n",
    "    print(f\"{bigram[0][0]} {bigram[0][1]}: {bigram[1]}\")  # Print bigram and its frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addbbe13",
   "metadata": {},
   "source": [
    "It seems like there are a lot of word pairs with 'I' which makes a lot of sense considering these are personal letters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6e1877",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging\n",
    "\n",
    "POS tagging (Part-of-Speech tagging) is the process of assigning a specific part-of-speech category (e.g., noun, verb, adjective, etc.) to each word in a sentence. This helps in understanding the grammatical structure of a sentence and is a key step in many natural language processing (NLP) tasks such as information extraction, machine translation, and text analysis.\n",
    "\n",
    "POS tags typically include categories like:\n",
    "\n",
    "- Nouns (e.g., \"dog\", \"city\")\n",
    "\n",
    "- Verbs (e.g., \"run\", \"is\")\n",
    "\n",
    "- Adjectives (e.g., \"quick\", \"beautiful\")\n",
    "\n",
    "- Adverbs (e.g., \"quickly\", \"very\")\n",
    "\n",
    "- Pronouns (e.g., \"he\", \"they\")\n",
    "\n",
    "- Prepositions (e.g., \"in\", \"on\")\n",
    "\n",
    "- Conjunctions (e.g., \"and\", \"but\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b47bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These libraries are used for tokenization and POS tagging\n",
    "\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger_eng') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd636c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_35 = montagu['body_cleaned'].iloc[34]\n",
    "print(letter_35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9d90e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of letter 35: {len(letter_35)} characters\")\n",
    "print(f\"Number of words in letter 35: {len(letter_35.split())} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed143c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_35_lower = letter_35.lower()\n",
    "unique_words = set(letter_35_lower.split())\n",
    "print(f\"Unique words in letter 35 (lowercase): {len(unique_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f8592",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(letter_35_lower)\n",
    "\n",
    "# Sentence tokenization results in a list of sentences\n",
    "print(f\"Number of sentences in letter 35: {len(sentences)}\")\n",
    "\n",
    "for sentence in sentences[:5]:\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fae0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentence for POS tagging\n",
    "sample_sentence = sentences[0]  # First sentence for demonstration\n",
    "\n",
    "# Apply POS tagging\n",
    "pos_tags = pos_tag(word_tokenize(sample_sentence))\n",
    "\n",
    "print(f\"POS Tags for the sample sentence: {pos_tags}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0069b3",
   "metadata": {},
   "source": [
    "### POS Tags in this example\n",
    "\n",
    "- **NN (Noun, Singular)**: Represents singular nouns.\n",
    "    - *advantage, weather, journey, summer, beauty, pleasure, prospects, meadows, flowers, herbs, air*\n",
    "\n",
    "- **VBP (Verb, Non-3rd Person Singular Present)**: A verb in the present tense used with subjects other than third-person singular (he/she/it).\n",
    "    - *have*\n",
    "\n",
    "- **VBN (Verb, Past Participle)**: A verb in past participle form, often used with auxiliary verbs.\n",
    "    - *had*\n",
    "\n",
    "- **DT (Determiner)**: A word that introduces a noun.\n",
    "    - *the, all*\n",
    "\n",
    "- **IN (Preposition/Subordinating Conjunction)**: Used to show relationships between elements in the sentence.\n",
    "    - *of, in, as*\n",
    "\n",
    "- **RB (Adverb)**: Modifies or describes a verb, adjective, or another adverb.\n",
    "    - *very, now*\n",
    "\n",
    "- **JJ (Adjective)**: Describes or modifies a noun.\n",
    "    - *fine, sweet*\n",
    "\n",
    "- **PRP$ (Possessive Pronoun)**: A pronoun that shows ownership or possession.\n",
    "    - *my, its*\n",
    "\n",
    "- **VBD (Verb, Past Tense)**: A verb in the simple past tense.\n",
    "    - *enjoyed, perfumed, pressed*\n",
    "\n",
    "- **CC (Coordinating Conjunction)**: Used to connect words, phrases, or clauses of equal importance.\n",
    "    - *and*\n",
    "\n",
    "- **PRP (Personal Pronoun)**: Refers to a person or thing.\n",
    "    - *I, it, them*\n",
    "\n",
    "- **NNS (Noun, Plural)**: Represents plural nouns.\n",
    "    - *prospects, meadows, sorts, flowers, herbs*\n",
    "\n",
    "- **. (Punctuation)**: Marks the end of a sentence or clause.\n",
    "    - *.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c08a59",
   "metadata": {},
   "source": [
    "### What can we do with POS Tagging?\n",
    "\n",
    "We can analyze the structure of the sentences, identify parts of speech, and even extract specific types of words.\n",
    "\n",
    "For example, we can extract all adjectives from this letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b785156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What can we do with POS Tagging?\n",
    "# We can analyze the structure of the sentences, identify parts of speech, and even extract specific types of words.\n",
    "# For example, we can extract all adjectives from this letter.\n",
    "\n",
    "# First we need to create a function that extracts adjectives from a sentence\n",
    "def extract_adjectives(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(words)\n",
    "    adjectives = [word for word, tag in pos_tags if tag.startswith('JJ')]  # 'JJ' is the tag for adjectives\n",
    "    return adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5661b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check to see that our function works\n",
    "sample_sentence = sentences[0]  # First sentence for demonstration\n",
    "adjectives = extract_adjectives(sample_sentence)\n",
    "print(f\"Adjectives in the sample sentence: {adjectives}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad630c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can improve our function by filtering out certain words that are not adjectives\n",
    "\n",
    "# Here we rely on a simple heuristic: if the word that is tagged as an adjective is followed by a noun, it is likely an adjective.\n",
    "\n",
    "def extract_adjectives_improved(sentence):\n",
    "    sentence = sentence.lower()  # Convert to lowercase for consistency\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged_words = pos_tag(words)\n",
    "    adjectives = []\n",
    "    for i in range(len(tagged_words) - 1):\n",
    "        current_word, current_tag = tagged_words[i]\n",
    "        next_word, next_tag = tagged_words[i + 1]\n",
    "        if current_tag.startswith('JJ') and next_tag.startswith('NN'):\n",
    "            adjectives.append(current_word)\n",
    "    return adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32d8048",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extract_adjectives_improved(sample_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8393ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get adjectives from all sentences in the letter\n",
    "all_adjectives = []\n",
    "for sentence in sentences:\n",
    "    adjectives = extract_adjectives_improved(sentence)\n",
    "    all_adjectives.extend(adjectives)\n",
    "\n",
    "print(all_adjectives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb51df",
   "metadata": {},
   "source": [
    "### Why extend and not append?\n",
    "\n",
    "Last week we learnt to use `append()` to add an element to a list. Now we are using `extend()` instead. Why?\n",
    "\n",
    "`append()` simply adds 'adjectives' in each iteration of the for loop to the list 'all_adjectives'. In this case 'adjectives' is also a list. This means that if we used `append()` we would be creating a list of lists. What we actually want is to create a list of strings, so we need to `extend()` our list 'all_adjectives' with the strings in the list 'adjectives' every time that list is created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the unique adjectives from the letter and count them\n",
    "print(f\"Unique adjectives in letter 35: {len(set(all_adjectives))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3b20ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see all the unique adjectives in the letter and their counts\n",
    "from collections import Counter\n",
    "adjective_counts = Counter(all_adjectives)\n",
    "\n",
    "# Print the most common 10 adjectives\n",
    "for adj in adjective_counts.most_common(10):\n",
    "    print(f\"{adj[0]}: {adj[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c8fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfect! Now let's do this for all letters in the dataset and save the results in a new column in the DataFrame\n",
    "montagu['adjectives'] = montagu['body_cleaned'].apply(lambda x: extract_adjectives_improved(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9cff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's get all the unique adjectives from all letters and their counts\n",
    "\n",
    "adjectives_all_letters = []\n",
    "for adjectives in montagu['adjectives']:\n",
    "    adjectives_all_letters.extend(adjectives)\n",
    "\n",
    "adjective_counts_all_letters = Counter(adjectives_all_letters)\n",
    "\n",
    "for adj in adjective_counts_all_letters.most_common(10):\n",
    "    print(f\"{adj[0]}: {adj[1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b01b036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the most common adjectives in a bar chart\n",
    "import matplotlib.pyplot as plt\n",
    "# Get the most common adjectives and their counts\n",
    "most_common_adjectives = adjective_counts_all_letters.most_common(10)\n",
    "adjectives, counts = zip(*most_common_adjectives)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(adjectives, counts, color='skyblue')\n",
    "plt.xlabel('Adjectives')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Most Common Adjectives in Montagu Letters')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4950a5",
   "metadata": {},
   "source": [
    "### Another Example with POS Tagging\n",
    "\n",
    "Let's extract nouns. Adjectives showed us hints about the sentiments in this corpus and nouns could get us closer to the topics of analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c538f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time, we will get POS tags for all letters and save them in a new column in the DataFrame\n",
    "montagu['pos_tags'] = montagu['body_cleaned'].apply(lambda x: pos_tag(word_tokenize(x)))\n",
    "# Let's take a look at the first few rows of the new column\n",
    "print(montagu[['body_cleaned', 'pos_tags']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf0b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remember what our data frame looks like\n",
    "print(montagu.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceed1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We did a lot! This is a good place to save our work\n",
    "montagu.to_csv('../Data/montagu/montagu_letters_v3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a89f23",
   "metadata": {},
   "source": [
    "At this point, we could actually split our dataset into two: one with metadata about the letters and one containing the analysis of letters. But more on that later!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab977f5",
   "metadata": {},
   "source": [
    "### Nouns\n",
    "\n",
    "Nouns in English are a bit more complicated than adjectives for POS tagging. Unlike other languages, adjectives in English are not declines. Nouns however are. \n",
    "\n",
    "- Nouns can be plural or singular, i.e. cat (NN) and cats (NNS)\n",
    "- There are proper nouns, both singular and plural: Belgrade (NNP) or Austrians (NNPS)\n",
    "- Nouns can take on a possesive ending, i.e. Julius Caeser's in Letter 3. Luckily NTLK removes the possesive ending\n",
    "('Julius', 'NNP'),\n",
    "('Caesar', 'NNP'),\n",
    "(\"'s\", 'POS'),\n",
    "\n",
    "More on that can be found in the [NLTK documentation](https://www.nltk.org/book/ch05.html)\n",
    "\n",
    "When we are extracting nouns, we have to make sure that we capture all of these nouns or intentionally leave some behind. What matters is that we don't forget about them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc51115",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']  # Singular and plural nouns, proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4a2af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find all nouns in the letters\n",
    "def extract_nouns(pos_tags):\n",
    "    nouns = [word for word, tag in pos_tags if tag in noun_tags]\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e291427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the DataFrame\n",
    "montagu['nouns'] = montagu['pos_tags'].apply(extract_nouns)\n",
    "# Let's take a look at the first few rows of the new column\n",
    "print(montagu[['body_cleaned', 'nouns']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01148475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean up the nouns by lowercasing them and removing non-alphabetic entries\n",
    "nouns = montagu['nouns']\n",
    "nouns = [noun.lower() for noun in nouns if noun.isalpha()]\n",
    "print(nouns[:10])  # Display the first 10 nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36b35f2",
   "metadata": {},
   "source": [
    "What is happening here? Why can't we do this transformation?\n",
    "Remember attribute errors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977c6b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what type of data structure we have\n",
    "print(type(nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4d263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if we just made it a list?\n",
    "nouns_list = nouns.tolist()  # Convert the Series to a list\n",
    "print(type(nouns_list))  # Check the type again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff271a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the first item in this list?\n",
    "# We want it to be a string\n",
    "print(type(nouns_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d5db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is a list?\n",
    "# We need to flatten the list of lists into a single list\n",
    "from itertools import chain\n",
    "nouns_flat = list(chain.from_iterable(nouns_list))  # Flatten the list of lists\n",
    "\n",
    "print(type(nouns_flat))  # Check the type again\n",
    "print(type(nouns_flat[0]))  # Display the first 10 nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc6b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes! Now we have a flattened list of nouns\n",
    "# Let's clean it up further and count them\n",
    "\n",
    "nouns_cleaned = [noun.lower() for noun in nouns_flat if noun.isalpha()]  # Remove any non-alphabetic entries\n",
    "print(len(nouns_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dc26d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more contraversial step could be to remove any words that are less than 3 characters long. \n",
    "# There are very few nouns in English that are one or two characters long.\n",
    "# Often in datasets like this, they are either OCR errors\n",
    "\n",
    "nouns_cleaned = [noun for noun in nouns_cleaned if len(noun) > 2]\n",
    "print(len(nouns_cleaned))  # Check the length after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47edd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's count them!\n",
    "noun_counts = Counter(nouns_cleaned)\n",
    "\n",
    "for noun, count in noun_counts.most_common(10):\n",
    "    print(f\"{noun}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf6a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the most common 50 nouns in a bar chart using seaborn\n",
    "# Get the most common nouns and their counts\n",
    "most_common_nouns = noun_counts.most_common(50)\n",
    "nouns, counts = zip(*most_common_nouns)\n",
    "plt.figure(figsize=(14, 8)) # slightly bigger figure size for better readability\n",
    "sns.barplot(x=list(nouns), y=list(counts), palette='viridis') # color palette for better aesthetics\n",
    "plt.xlabel('Nouns')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Most Common Nouns in Montagu Letters')\n",
    "plt.xticks(rotation=90) # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17499093",
   "metadata": {},
   "source": [
    "Nouns already tell a different story about our data. In many ways, they get us closer to the contents of these texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69b956f",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83bf84",
   "metadata": {},
   "source": [
    "When we take a look at the top 50 words, what we see is that there are some plural and singular nouns. For example men and man both appear in this data. \n",
    "\n",
    "In some cases, this kind of information is super super useful. In other cases however we might want to turn all nouns into their singular forms to count them.\n",
    "\n",
    "More generally, this is a text normalization task called lemmatization. Lemmatization is the task of turning inflected words into their dictionary forms. An alternative, more of a shortcut version of this task is stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e49126",
   "metadata": {},
   "source": [
    "[What's the difference between stemming and lemmatizing?](https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming)\n",
    "\n",
    "\"**Stemming** usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. **Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma. \n",
    "\n",
    "If confronted with the token \"saw\", **stemming** might return just \"s\", whereas **lemmatization** would attempt to return either \"see\" or \"saw\" depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma.\" See [this post](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5417b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a2def6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will do this with spaCy https://spacy.io/ and https://spacy.io/api/lemmatizer\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\"])  # Disable NER and text classification for speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68df0a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the first letter from the dataset to test our spaCy implementation\n",
    "letter_1 = montagu['body_cleaned'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e027adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(letter_1)  # Process the letter with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45170644",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[:10]  # Display the first 10 tokens in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c51dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the word at index 13 in the letter and 16 in the lemma\n",
    "# they are the same word, the difference is because of the comma\n",
    "print(letter_1.split()[13])  # Display the word at index 16\n",
    "print(doc[16].lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1f9f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get all our lemmas\n",
    "lemmatized_tokens = [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec3df0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a tuple of the original word and its lemmatized form\n",
    "lemmatized_pairs = [(token.text, token.lemma_) for token in doc if token.is_alpha]\n",
    "# Display the first 10 lemmatized pairs\n",
    "print(lemmatized_pairs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b5f1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the one that have changed side by side for better readability\n",
    "for original, lemma in lemmatized_pairs:\n",
    "    if original != lemma:  # Only print pairs where the original and lemma are different\n",
    "        print(f\"{original} -> {lemma}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dighum101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
