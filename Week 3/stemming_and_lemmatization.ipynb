{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4bbb18a",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization\n",
    "\n",
    "One common problem with standardizing text is standardizing all parts of a word to its root, stem, or prefix. This is useful as it allows us to analyze word meaning without having to pour over separate inflectional forms of a word. It also speeds up the process as we have fewer words.\n",
    "\n",
    "[What's the difference between stemming and lemmatizing?](https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming)\n",
    "\n",
    "\"**Stemming** usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. **Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma. \n",
    "\n",
    "If confronted with the token \"saw\", **stemming** might return just \"s\", whereas **lemmatization** would attempt to return either \"see\" or \"saw\" depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma.\" See [this post](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) for more information.\n",
    "\n",
    "So what do we do? Use pretrained models from [spaCy](https://spacy.io/)! It does all the tokenization and lemmatization for you.\n",
    "\n",
    "[Read more about spaCy's pretrained models](https://spacy.io/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c9e29b",
   "metadata": {},
   "source": [
    "# Lemmatizing with spaCy\n",
    "\n",
    "Let's start by loading the trained model. We don't need the Named Entity Recognition (NER) or the text classification capabilities of the model, so we don't load them to make everything faster. If we wanted to lemmatize a language other than English, we would just need to download a trained model for that language using spaCy and change the 'en_core_web_sm' to the name of that model. Everything else would be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6790df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the small pretrained model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"textcat\"])\n",
    "print(type(nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a046423",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# spaCy expects a string as input, so let's use .join to force our list into a string  \n",
    "words = ' '.join(tokens)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f144e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(words)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6984d786",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We can now get the lemma of any word using the .lemma_ attribute\n",
    "doc[5].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129b1d4f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ...Or even the part of speech\n",
    "doc[5].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f2633",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Note that spaCy also has its own stopwords list\n",
    "nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e8b339",
   "metadata": {},
   "source": [
    "Check out the [spaCy documentation](https://spacy.io/api/token#attributes) for more information about all the linguistic features that spaCy allows you to access as attributes.\n",
    "\n",
    "Now, let's create a function that takes in a list of tokens and lemmatizes it using spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48f6801",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define our function\n",
    "def lemmatize(tokens):\n",
    "    \"\"\"Return the lemmas for each word in `tokens`.\"\"\"\n",
    "    \n",
    "    # spacy models operate on strings, not lists, so we turn the tokens back into\n",
    "    # a string of words\n",
    "    words = ' '.join(tokens)\n",
    "    \n",
    "    # this line does all sorts of processing, including the lemmatization.\n",
    "    # `doc` will be like a list of tokens that we can iterate over\n",
    "    doc = nlp(words)\n",
    "    \n",
    "    # each token in `doc` holds information about that token. The `lemma_`\n",
    "    # attribute holds the lemma of that token represented as a string. For\n",
    "    # performance reasons, the `lemma` (without the trailing underscore) holds\n",
    "    # an integer representation of the token, that we'll rarely ever need.\n",
    "    return [token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4270756",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "lemmas = lemmatize(tokens)\n",
    "print(lemmas)\n",
    "\n",
    "# Notice that spacy lemmatizes pronouns (e.g. \"you\", \"I\", \"your\") in a funny way.\n",
    "# It just tells us that they are pronouns, rather than giving us something like\n",
    "# \"your\" -> \"you\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2854078",
   "metadata": {},
   "source": [
    "# N-grams, skip-grams, and BERT?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e0a024",
   "metadata": {},
   "source": [
    "Are you interested in tokenizing more than just single words for the purpose of increasing \"context\"? [n-grams](https://en.wikipedia.org/wiki/N-gram) are \"contiguous sequence of n items from a given sample of text or speech.\"\n",
    "\n",
    "[Check out this clever solution for n-gramizing text](https://stackoverflow.com/questions/17531684/n-grams-in-python-four-five-six-grams)\n",
    "\n",
    "Do you want even more context? We will learn more about [skip-grams](https://en.wikipedia.org/wiki/Word2vec) and [BERT](https://www.searchenginejournal.com/google-bert-update/332161/#close) later in this course. \n",
    "\n",
    "\n",
    "\"The skip-gram architecture weighs nearby context words more heavily than more distant context words.\"\n",
    "\n",
    "\"The BERT algorithm (Bidirectional Encoder Representations from Transformers) is a deep learning algorithm related to natural language processing. It helps a machine to understand what words in a sentence mean, but with all the nuances of context.\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
